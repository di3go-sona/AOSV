#### Interrupt Management - (24-04-2020)

Let's start with a picture that we have already seen, to recap. 

<img src="C:.\images\IDT and GDT relations.png" alt="IDT and GDT relations" style="zoom:67%;" />

This is the relation between the IDT and the GDT. The IDT, when running in Protected Mode, is a table composed by only 256 entries. Each entry is called Interrupt Vector, it is associated which an Interrupt Vector Number which is the index entry of that table. Every time we want to serve a specific interrupt, we look its IVNumber an then we use that number to displace the IDT. Remember that in Real Mode, instead of IDT, we rely on the IVT (located traditionally at address 0x0..0 which in each entry stores directly a function pointer, the IDT stores a IDTEntry which is composed by a selector and an offset. The selector tells us that the base address of the code which we are interested in is stored into the GDT. We can use that selector to displace the GDT and then after security checks are passed correctly we can find the address of the interrupt handler which we want to activate using the offset stored in the IDTEntry starting from the base address retrieved through the GDT. 

Since the number of entries inside the IDT is limited, it is a common use to register as interrupt handlers such routine which perform some kind of dispatching actions (like the syscall dispatcher of course). So there are a lot of information which are required like the device number or the syscall number. Inside the 256 entries we have a lot of different types of interrupt: there are traps, Inter-Processor interrupts and also hardware interrupts. 

Let's take a look inside an IDT Entry on 64 bit systems.

<img src=".\images\IDT Entries x64.png" alt="IDT Entries x64" style="zoom:80%;" />

We rely on Interrupt/Trap Gates cause we don't know when an interrupt or a trap will be generated. It can happen (usually) that when we are running at Ring 3 an hardware interrupt arises, so this entries must allow also the passage from Ring 3 to Ring 0 execution mode. Inside the entry we have of course the already discussed Segment Selector (16 bits like a Real Mode address -> Remember that), we have an overall of 64 bits of offset split among different portions (just hardware optimizations), we have also some control bits like the Presence bit, the DPL which specifies the Descriptor privilege level of the associated selector, the Type (more well explained later) and the IST Interrupt Stack Table (a number from 0 to 7, if 0 it will be used the normal kernel stack, otherwise the other available stacks in the TSS). An entry is managed code-side by the gate_struct struct (each field takes a member in the struct). It is declared as packed which means that we don't want alignment padding inside. The control bits are all together represented in a idt_bits struct in which there are set of bits identified by a name, we can access fields in a bit-level granularity (follia pura che C permetta questo). Of course the struct is packed.

To populate an IDT Entry until Kernel version 5 there was a specific tailored function to do that, now on the kernel relies on scripts and macros that do a lot of job at compile time. This is why call a function for each IDT entry at bootstrap time was too costly but this function is still available if our code must interact with IDT entries.

<img src=".\images\IDT Entry Initialization.png" alt="IDT Entry Initialization" style="zoom:80%;" />

The function is called pack_gate which pack all the necessary information in a gate_desc struct. The address "func" must be split into different portions in order to populate accordingly the firmware. Notice that the segment which is used always in 64 bit mode is the KERNEL_CS cause we have a lot of memory and with an offset of 64 bit we can select all the kernel code (flat segment mode), otherwise the segment must be passed as input. The rest of fields are propagated from the input passed ones.

From 0 to 31, in the IDT, we have vectors associated to Non-Maskable interrupts and exceptions (from 20 to 31 are Intel-reserved). It means that our SO must install in that entries an exception handler which will be activated each time some conditions are presented into the firmware. This is a sort of handshake between firmware and software in order to let things work properly. An example is the dereferencing of a NULL pointer which will generated an exception managed by the associated handler. After we have from 32 to 127 and from 129 to 238  some available vectors to manage External Interrupts (IRQs). It means that I/O devices can arises some requests specifying an integer between that range. This is a Linux decision, instead the first ones are shared across different OS cause it is a Intel requirement. The number 128 is associated to the software trap scheme used for the syscalls. The number 239 is associated with the Local APIC Timer. The LAPIC circuitry keeps track of time passing and also nowadays a specific interrupt is managed by relying on that firmware component, it used for example by the scheduler in order to know if a quantum time of a process is expired or not. From 240 to 250 there are free entries reserved by Linux for future use. The last 4 entries, from 251 to 255, are associated with Inter Processor interrupts.

A gate descriptor is a segment descriptor of type system. It means that its type depends on what kind of kernel facility it points. We have already mentioned Trap-gate descriptors. There are also Task-gate descriptors which were meant to support multi tasking on Intel cpu. The Call-gate descriptors which are not used (they allow to perform a long call from userspace to kernel space). Then we have interrupt-gate descriptors. Depending on what type is the descriptor referenced by the IDT Entry, we have a different value for the field type: 0xE for gate-interrupt, 0xf for gate-trap, 0xC for gate-call and 0x5 for gate-task. From this type, the firmware will execute different tasks. 

But what are the real different between an Interrupt and a Trap (both firmware-side and OS-side)? First of all interrupt is an asynchronous event, instead a trap (or exception) is a synchronous one. Async means that it can be delay, it can be related to a previous completed task and so on.  The interrupts are usually generated by external devices and they can be masked or not (NMI). Instead sync means that the event is generated by the CPU itself while running some portion of code. For example an exception rise if the CPU tries to divide a number by zero. 

For the interrupts, it is automatically reset the interruptible-state of a CPU core relying on the IF bit, because otherwise there is the possibility to overflow the stack related to that kind of interrupt cause we would always activated the very same handler until someone clears the interrupt request. Instead for the trap mechanism this is not true, in fact critical sections in trap handlers must explicitly mask and the re-enable interrupts by using CLI and STI asm instructions. A trap is a sync event, so there is no need to automatically clear interrupts before activating a trap handler, we don't expect no other same trap requests while performing the associated handler! But clearing interrupt could not be enough in a multi-core system cause, even if we cannot be interrupted, another CPU could interfere within our critical section. Notice that the FLAGS register is a per-cpu register. Cause this a lot of trap handlers rely on a large family of spin lock in order to enhance the correctness (atomicity) while handling the trap.

We have in general 3 different classes of interrupts: the I/O family, which is generated every time an I/O device requests attention to the kernel and the interrupt handler must query the device to setup the proper actions (this is a sort of handshake between driver and devices due to the limited number of IDT entries), the Timer family, which is related to the LAPIC times which issues interrupts to notify kernel that a certain amount of time has passed, then the Interprocessor family, which are used to synchronize different CPUs while it is performed a specific task in some circumstances (like the INIT-SIPI-SIPI boot sequence).  

Differently a sync call to a function f in which it is saved onto the stack only the return address in order to restore the previous execution context (cause it is a sync event), before handling an interrupt it is needed to save at least more the FLAGS register! So we have to understand what the firmware does. Some actions are performed by all the architecture, others are performed from a subset of them. The amount of information that the firmware saves before handling an interrupt is called Interrupt Frame. The Interrupt Frame is saved onto the switched stack (not the user one, but the one specified in the IDT entry). 

<img src=".\images\Interrupt Frame.png" alt="Interrupt Frame" style="zoom:80%;" />

If we are coming from a different context (like the user one), the firmware will save onto the stack the SS and the SP of the caller context in order to be able to restore it at the end of the routine. Then it saves the FLAGS register to restore correctly branches status. In both cases, if we entered the interrupt handler from ring 3 or ring 0, the firmware will push the CS onto the stack. This is very important cause in this case we can, just reading it, know what is the previous caller context. Obv we have also the return address onto the stack identified by the RIP. The last field is an optional one, it is associated either the error code (for the exception) or the vector number (number of I/O device). It depends on the type of interrupt which is served. This is the lowest part of the already defined PT_REGS struct! 

So the global activation scheme is the following. First of all we look for the stub code in both cases of trap or interrupt, the stub performs a sort of leveling of information pushed onto the stack. Why this? Cause for example, the last value of the Interrupt Frame is optional! So in order to abstract these differences, the kernel will put there another value in order to have always the same amount of information available. There is a stub for each vector! After making uniform the Interrupt Stack Frame, it jumps into a common dispatcher. It will performs some common action like complete onto the stack the population of the PT_REGS struct (it saves all the CPU context), switch also the GS and if it is required change possibly the stack. At the end it will call the actual handler which will manage the specific interrupt and then it will return by mean a regular ret instruction to the dispatcher. The dispatcher must clean all the data written onto the stack so it uses an iret instruction which deletes all the interrupt stack frame and returns to the RIP address previously saved.

How does stub manage the last value of interrupt frame? If it is an exception, it pushes the error code, if the error code is not specified it pushes a dummy -1 value. If it is an IRQ, the vector number is pushed. At this point the dispatcher is uniformly reached. 

<img src=".\images\Exception Examples.png" alt="Exception Examples" style="zoom:80%;" />

This is the code associated to exceptions stub in one of the earliest kernel version. It was simplified. For each routine (overflow or general_protection or page_fault in the example) there is a different entry point. In the first two cases the stub pushes a negative value by hand cause we don't have an error code. The page fault exception instead has an error code pushed automatically by the firmware. At the end the stub jumps into the dispatcher. Modern version of the kernel significantly relies on macros.

<img src=".\images\Stub macro way.png" alt="Stub macro way" style="zoom:80%;" />

At compile time this macro populate, for each external IRQ (from 0x80 to 0xEE, 128-238), the corresponding Stub entry whit the same amount of instructions. In fact starting from the irq_entries_start symbol, thanks to the offset specified by the interrupt vector number, it is easy to reach the corresponding stub entry. This macro makes a loop on all the external vectors until the first system vector is reached. For each of these vectors it pushes the vector number (the index loop + 0x80) and jump into the common_interrupt symbol which points to the common dispatcher. It is said that the macro packs 1 stub into 8-byte block for each of these vectors. So in the IDT the offset will be the irq_entries_start symbol address + the offset to reach a specific stub.

Also exception stubs are generated relying on macros, but in this case we can't use a loop cause the presence of the error code is related to the specific exception.

<img src=".\images\Exceptions stub macro way.png" alt="Exceptions stub macro way" style="zoom: 80%;" />

This macro will generate the stub for each "sym" exception. Every "sym" will call the "do_sym" function. For example the overflow is associated to an entry point of overflow stub which will call at the end the do_overflow kernel function. The presence of the error code is passed as a boolean as a input parameter for the macro. If it is false the macro pushes the -1 value. Then it moves the RSP into RDI, it means that it moves the pt_reg pointer into the first argument of the function and if it has an error code, it is passed as the second argument to the function within RSI. 

SWAPGS is an asm instruction which is intended to be used by only the OS to switch between two Model Specific Registers. We have the IA32_GS_BASE register which points to the user-mode per-thread data structure (on Windows, Linux uses FS) and the IA32_KERNEL_GS_BASE which points to the kernel per-CPU data structure. This is the fastest way to access kernel per-CPU data structures while changing the context. Now we can look an easier implementation of the dispatcher. 

<img src=".\images\Exception Dispatcher skeleton.png" alt="Exception Dispatcher skeleton" style="zoom:80%;" />

The first instruction clears the direction flag cause to check if it was set previously is to costly and we want to have a common direction in this scheme. The direction flag is set by string operators but i really don't understand why in this case it cares of that but who knows :D. The second operation is really important, the dispatcher accesses the base address of Interrupt Stack Frame + an offset. Such offset points to the CS field which differentiate between kernel CS and user CS. Via test asm instruction, which performs a bitwise and operation of 11 (bit representation of number 3) and the CPL field of the caller code segment ) and check if it is 0 the result, the dispatcher knows if there was a context switch from user space and so if it is needed a swapgs. After that it performs a CPU snapshot that completes the preparation of PT_REGS struct. Then it prepares parameters and finally calls the actual handler. If it is an interrupt that needs to be served, the dispatcher is performed with interrupts disabled. So the actual handler could set again interrupt cause the kernel is preemptable. So when the actual handler returns, the first thing to do is to disable again interrupt, cause we don't want to be interrupted while restoring the previous execution context!

<img src=".\images\PT_REGS x64.png" alt="PT_REGS x64" style="zoom:80%;" />

This is the PT_REGS struct for 64 bit systems. Notice that the orig_ax means differently for different type of interrupt.

Notice that, in the dispatcher code there is a conditional branch before the SWAPGS instruction. This means that also that conditional branch can be fake due to a spectre attack. The goal of a speculative attack on gs segment is to execute kernel code with a gs set by the user. This allow users to bypass KASLR or leak actual kernel memory. In order to perform this kind of attacks we need a specific gadget within the kernel code: immediately after the SWAPGS instruction, it must be a memory access from/to it. Linux doesn't have gadget like that, but Windows yes. It can be found in windows kernel code a code snippet associated to an exception handler, in which after the check on the need to swap the gs segment, there are some accesses to the kernel gs to registers. These accesses bring kernel memory in cache, so we can find certain fingerprints of kernel code in order to leak memory and/or addresses. 

Let's look now to the Page Fault Handler. It is associated to an error code and it is activated every time that a v2p translation can't be carried out. Notice that a page fault could be associated also a minor faults like the one that arises each time the kernel is lazy to allocate memory for userspace applications. So it must be able to manage those different situations. The page fault handler is declared as we said like a DO_PAGE_FAULT(struct pt_regs *regs, ul error_code). It is perfectly compliant to the "macro way" exceptions entry point generation seen so far. In the error code the first three bits are meaningful: bit 0 can be 0 if no page was found or 1 if there was a protection fault but the v2p is good, bit 1 tells that if it is 0 the access was a read, 1 otherwise was a write, bit 3 tells if 0 the fault happened in kernel mode, 1 otherwise in user mode. In kernel mode if there is a page fault while running the page fault handler to resolve a previous fault, it is triggered a double fault handler, in which if there is another page fault, things are going so bad so the kernel performs an hard reset. 

But how can the operating system know what are the address which is faulty and the address of the instruction that tried to do the bad operation? So, before calling into the handler, the firmware creates the interrupt stack frame in which is stored among the others the RIP pointer (it means the virtual address of the user code which generates the fault). What about the faulty address? This information is so important that there is a specific Control Register associated to it: CR2, in fact, contains the unaccessible memory address which generates the fault, it is set automatically by the firmware and kernel must exploit it in order to retrieve that fundamental information.

What happens if there is a page fault during kernel mode? We already said that thanks to the facility copy_from_user() kernel can work on some memory address passed by an untrusted user. Kernel relies on verify_area() and access_ok() facilities to know if the access to that memory address is safe or not. But this check requires some time and it takes place quite often. In modern version of the kernel, access_ok is not more involved during a copy_from_user call, cause kernel prefers to pay to manage a page fault in kernel mode than to pay each time the cost of the check. This is why the probability that an user makes something bad without consciousness is low. So in the page fault handler a lot of cases are managed: if the VA stored in CR2 is a valid one, the kernel must search for that page in the disk (cause it was swapped) or it must allocate physical memory due to its lazy approach, otherwise it jumps into a bad_area section in which it tries to activate a FIXUP routine in order to come back in a working state and continue its execution. In this bad_area section, kernel overwrites the pt_regs->rip information (previously pushed) with the address of the fixup routine, faking so the dispatcher to return to a different routine wrt the caller one. Obv the fixup is executable only in kernel mode. It is defined by macros (very terrible to read).

<img src=".\images\Fixup Assembly.png" alt="Fixup Assembly" style="zoom:80%;" />

This is the real assembly which is generated by the macro for the get_user() function which allow to read data from user to kernel. The macro was based on the size of the data that the kernel is trying to read from a user buffer. In this case the label 1 is the real access to the user memory. EBX contains the address from which kernel is trying to read a single byte. After that there are two non standard sections, they mean that the following code must be placed in a different place called for the first fixup for the second \_\_ex\_table. It means that the following instruction of 1 is the label L1423. So first of all we have to notice that exists a table of the exceptions composed by two integer for each entry: the first one is the faulty address and the second one is the fixup address. In our example the section \_\_ex\_table will write in the table that if there is a page fault for the address identified by the label 1, the fixup routine that must be activated is the one at the address of the label 3 (which will be placed in a different part of the executable space). So if the access to EBX  is wrong, it will be activated the fixup. The first thing that the fixup does is to set an error code into EAX (-14 == -EFAULT), then in zeros the DL register (the one in which there was the read value) and the it returns manually via jump to the address identified by the label 2. BUT since the next instruction is a non standard section, it will return to the label L1423. What a trip!

How do interrupts work on multi-core machines? It can happen that in a multi core system, two or more different thread of the same application run on different cores, but the interrupt/trap is delivered only to one core. This situation can bring the system in a inconsistent state, so we need to propagate the interrupt/trap event to other cores, id needed. For example, in a single core machine, if an userspace application calls the free() libc function which could call in the very end a munmap() in order to ask to SO to free a chunk of memory that we no longer need (this is a trap in 32 bit system).  This deals with v2p translation and of course with the TLB too. So the SO is aware of that and after explicitly changing the page table, it will invalid partial the TLB. After returning in user mode that portion of memory will be no longer accessible and the state of the only thread (or more threads) of execution is consistent. The same example brings some problem in a multi core system. We know that the TLB is a per core cache. What if another thread, after the unmap operation of the first thread, accesses an address which is contained in the memory region which have been unmapped? Since the translation of that address is still cached in the TLB of the other core, if the address is assigned in the meanwhile to another application, it could potentially leak data of another application and modify it. This is no permitted since the isolation policy between processes in the system. So the system is in an inconsistent state and kernel must solved this situation. 

This is the reason of the existence of Inter Processor Interrupts. We have already seen them in the startup phase of the kernel and they are useful in order to synchronize different cores among certain activities. IPIs are interrupts that trigger execution of specific kernel routines on other cores. They enforce cross-core activities via a request/reply protocol, in order to trigger changes in the state of other cores. IPIs are generated synchronously at firmware level but processed asynchronously at software level. We have at least two priority levels are available: high and Low. High priority leads to immediate processing of the IPI at the recipient and low priority generally leads to queuing the requests and process them in a serialized way.  What is the hardware support to implement IPIs mechanism? We have already seen the registers to trigger IPIs. We have an interface to the APIC/LAPIC circuitry, LAPIC offers an instance local to each core. We know that the modern machines rely on a QuickPath interconnection on which there is built a virtual network dedicated (a general buffer and a per core network interface dedicated ) to the APIC interface on which IPI requests travel. An immediate handling is allowed only when there is no need to share data across cores, cause the protocol is a boolean one and doesn't allow to exploit some payload to share some data. An example is the system halt command which is sent among all cores upon panic situation in the kernel, it can and must be immediately executed! 

The IPI Vectors are the last in the IDT. First we have CALL_FUNCTION_VECTOR (vector 0xfb) which send an IPI message to all CPUs but the sender, forcing those CPUs to run a function passed by the sender. The corresponding interrupt handler is call_function_interrupt(). Usually this kind of request is performed by the smp_call_function() facility in which we can specify a pointer to a function what must be executed from all the other cores. The interrupt handler on each core will look at that parameter and it will execute that routine (we still don't know how this parameter is passed). Then we have RESCHEDULE_VECTOR (vector 0xfc) which is an old vector. This IPI asks a CPU to reschedule itself and the corresponding handler called reschedule_interrupt() only acknowledges the interrupt (we will come to it later). Another one is INVALIDATE_TLB_VECTOR (vector 0xfd) which allow a core to ask other cores to flush selectively their own TLB in order to return to userspace in a consistent state among all the system. This is so fundamental which required a dedicated entry in the IDT. The corresponding handler is called invalidate_interrupt(). 

The IPIs' API are wrapped in a struct called APIC. Store in this way the function pointers is better than call directly the API since kernel could change something in the background implementation. The first facility is default_send_IPI_all() which sends an IPI to all CPUs (including the sender), then we have default_send_IPI_allbutself() which sends an IPI to all CPUs except the sender, then there is default_send_IPI_self() which obv sends an IPI to the sender CPU and finally there is default_send_IPI_mask() which sends an IPI to a group of CPUs specified by a bit mask. 

The low priority IPI is incarnate in the CALL_FUNCTION_VECTOR vector which allows kernel to execute the same routine on different cores for sync purpose. How can we register that function and its parameters using just one vector? In older version of the kernel there were two global pointers: one for specifying the function and one to a struct which contains the arguments. In fact the smp_function_call() took a function pointer and a struct args pointer as parameters which were registered in the global pointers. Other cores retrieved parameters globally. Of course this organization can bring to a race condition among processors which want to send an IPI message to others (quite often if we think about the unmap example). So the global pointers were locked through a spin lock. So in order to set those global variable a core could spin a large amount of time decreasing drastically the performance. In kernel version 5.0 were introduced per-CPU linked list of registered functions and associated data. Now each CPU has a private queue from which it can pop element in a FIFO order in order to know what kind of function it must execute and the parameters that it requires. Two or more CPUs can access concurrently to these lists cause they rely on the lock-free list structure. So kernel abuses the per-cpu variable mechanism in order to allow to a single CPU to access in write mode to a per-cpu list of other cores!

<img src=".\images\Registering IPI Functions.png" alt="Registering IPI Functions" style="zoom:80%;" />

This image shows how the core 0 set the correct function and its parameters to all CPUs accessing their per cpu list in a non blocking way. After this operation it triggers the IPI and it could wait until all the n-1 cores have finished the execution of that function. After that it is sure that the system is in a consistent way and then it can give back control for example to the user.

The struct which is used in the list is called \_\_call\_single\_data. It contains a llist_node (in order to chain this struct to the lock free list of each cpu), a pointer to a function and a pointer to some arguments, it has some flags too. These entries are retrieved by a function called flush_smp_call_function_queue() which is executed by the generic IPI handler associated to that vector or if a CPU is about to go offline (before to do that it must handle all the queued IPIs).

Let's look at the simplified code of smp_call_function_many routine.

<img src=".\images\smp_call_function_many.png" alt="smp_call_function_many" style="zoom:80%;" />

Its first parameters is a bit mask of CPUs which should receive this IPI. Then we pass a function pointer and one to its parameters. Finally we have a wait boolean flag which tells that I will wait until others CPUs don't finish to handle my IPI request. There are two very important comments in this piece of code: first of all preemption must be disable when calling this function and then it can happen a deadlock when called with interrupts disabled. In this last case the code generates explicitly a warning to advice about that.  For each cpu specified in the cpu mask,  we take a pointer to the call_single_data struct of that cpu, we lock that pointer cause we are populating that list and the if the wait boolean is true we set some synchronous flag in order to notify that this operation must be done synchronously obv. Then we populate the struct with the function pointer and the parameters one. Then we push that structure in the corresponding list of the cpu and if this operation is executed correctly we can send the IPI message to that cpu. 

<img src=".\images\smp_call_function_many1.png" alt="smp_call_function_many1" style="zoom:80%;" />

At this point we can send the IPI message relying on the LAPIC controller and if the wait boolean is specified we check until all the involved CPUs have actually processed the IPI looking at the sticky flag added previously while adding a new node to each cpu list. Preemption is needed cause if we are descheduled we can have problem while accessing per cpu variables and because we are waiting that other cores does something (we cannot wait a core running in that core lol). If interrupt are disabled it could happen that another core registered a IPI in mine per cpu list and it is waiting for my execution of that task while i actually waiting it for mine -> tricky deadlock.  Obv the arch_send_call_function_ipi_mask function is an architecture independent one, it will in the very end call an architecture dependent one called \_\_default\_send\_IPI\_dest\_field which will set properly the ICR part 1 and 2 (it is split in two part remember). 

The last large family which remains to discuss about is the I/O Interrupts (Hardware). Devices continuously send interrupt to ask attention to the operating system, the number is very huge. So the SO must be as fast as possible to serve these Hardware Interrupts in order to avoid non responsive situations or huge idle time of devices waiting for its response. Remember that when manage an IRQ firmware set automatically the IF to zero!

We can distinguish among three part in the lifetime of an hw interrupt. There are critical actions which are required to be served with interrupts disabled, we ack the interrupt, reprogram the device and exchange data with it. Then there are non critical actions which can be managed with interrupts enable, it means that we have already collected all the critical information about the IRQ e now if something more important happens we can give attention to it. In these non critical actions we can manage data structures in the kernel which are not shared with the device. Notice that the set of non critical actions are executed immediately after the critical ones with only the possibility to be interrupted. At the end, after interacting with the device and publishing the required info in kernel data structures, eventually we can serve this IRQ, this is the reason behind the third part: non critical DEFERRABLE actions.

<img src=".\images\Performing Critical Actions.png" alt="Performing Critical Actions" style="zoom:80%;" />

This is a recap scheme to understand better how are performing the critical actions. First of all the device arise a hw interrupt request, then the LAPIC controller notify the OS which reads the IRQ number (index in the IDT, remember that it will be computed as a 32+IRQ cause first 32 entries are reserved), and pushes it onto the stack. Then in the corresponding vector there will be a routine which will search among all the registered actions the IRQ related one. This is why kernel allows to register more than one driver routines for a single IRQ number. During the exchanging of data in critical actions, the necessary information will be given to the kernel in order to understand what is the correct routine which must be activated to serve a specific device.

<img src=".\images\Private Thread Stack and IRQ Context.png" alt="Private Thread Stack &amp; IRQ Context" style="zoom:80%;" />

This is a snippet of code which is activated before calling into do_IRQ in order to perform the context switch. Notice that kernel decides to serve IRQs onto different stack properly set and retrieved by its own per cpu variables. First of all it checks if there is the need to swap gs (if the previous context was an user one), then it changes the stack. First, it saves onto the stack the content of the RDI value after used in this case to store the actual stack. Then it reads its own interrupt stack and it set that value in the RSP register. Abusing the push instruction it can perform a mem 2 mem write, writing onto the new stack the values stored by the firmware in the original stack. Then it restores the RDI content. After it set up the new stack context it pushes RSI content (IRQ number, as the convention says) and prepare the CPU snapshot. After it has prepared the parameters correctly, it calls do_IRQ.

What does the Deferred Work concept mean? The deferrable non critical actions can be done whenever the SO is available. Since it is not important, when to do the work exactly? First of all, we must underline that after the do_IRQ call, it is performed a IRET instruction, it means that all the job that is required to be done in the Interrupt Context is already completed. This allow us to manage the deferred part in another regular execution context, and therefore shift it in time. Since these works are delayed, we can aggregate them in a many-to-one relation in order to serve them all once a time. Obv we have to avoid starvation between priority different levels among deferred works. This approach is called Temporal Reconciliation. 

<img src=".\images\Deferred Work.png" alt="Deferred Work" style="zoom:80%;" />

This is a scheme in which it is explained how SO relies to Deferred Work approach. Think about a situation in which a thread grabs lock for entering in a critical section. During this time of period other threads are spinning until the first thread will release the lock (user or kernel application, the reason doesn't change). Now, while executing the critical section the CPU receive a several IRQs. The possibility are two: if the kernel used a complete serving approach, it would enlarge the duration of the critical section in a way in which the other threads would waste the major part of its CPU time, otherwise this approach allows SO to be as fast as possible also wrt to user application! It decides to perform critical and non critical (but not deferrable) actions as soon as the IRQ is received, then it continues executing the critical section with the possibility to receive another IRQ (so without loosing them). After releasing the lock, other threads are free to get it and when it found a CONVENIENT reconciliation point, the kernel will execute all the remaining deferred works!  

<img src=".\images\Top-Bottom Halves Driver.png" alt="Top-Bottom Halves Driver" style="zoom:80%;" />

This is a scheme which explains how a driver must be overall designed. A driver must care about the deferred work approach used by the operating system. So it must be written dividing the overall tasks in two different halves: the top-half ones which must be executed as soon the IRQ is delivered, and the bottom-half which could be deferred. In the top-half there is the minimal amount of work which is mandatory to later finalize the whole interrupt management and it is managed according to a non interruptible scheme. The bottom-half instead is the deferred finalization and it is scheduled by the top-half cause the top-half added the bottom-half into a proper queue data structure.

The main system in the modern kernel linux versions which deals with deferred work is SoftIRQs. It is called in this way cause it manages only the bottom-half part of a driver, the top-half is the related hardware part managed relying on IDT and vectors. This is a very critical system of the kernel linux. We know that we can receive an indefinite amount of hardware interrupts and it is a used practice that the software part is managed by the same CPU which received the hardware one. This is related to cache performance. So a CPU could be in charge to solve an huge number of deferred works and there is no consideration about the others tasks associated to the CPU cause if a reconciliation point is reached, the CPU must solved all of that softIRQs! There is no control on that. But when a reconciliation point is considered good? Well, it can be fired for example immediately after coming back from an hard IRQ (with IF=1), or while coming back from a syscall or moreover we can explicitly tells that after releasing a spin lock (leaving a critical section) if there is needed we can now execute some deferred works. The latter case is a sort of sync way in which programmers find suitable situations according to possible scheduling management, relying on specific routines (like spin_unlock_bh where bh stands for bottom-half), in which explicitly activate deferred works. But the most important thing to underline is: SoftIRQs are interruptible and they must be reentrant (able to work concurrently on different CPUs)! This means that they must take care of the situation of the "random" context in which they will be executed. In which way? With saving and restoring RFLAGS register. For example, a function f which is actually running on our CPU explicitly disabled interrupts (IF = 0) and then call our function g which contains its own critical section. Function g disables and then re enables interrupts, finally returns. While returning with local IF set to 1, it practically broke the caller function! So the we must ensure that the previous RFLAGS content must be restored! We can do that using local_irq_save(ul flags) and local_irq_restore(ul flags) which first of all save the original content of RFLAGS in an 64 bit local variable used at the end to restore the previous context. In this way we can compose safely functions without breaking caller contexts. Notice that SoftIRQs are not used directly, it is preferred to use Tasklets.

How can we implement the SoftIRQs mechanism?

<img src=".\images\SoftIRQs.png" alt="SoftIRQs" style="zoom:80%;" />

For each CPU we have a bit map called irq_stat[cpu].\_\_softirq_pending in which we can notify to that CPU if some deferred works is pending. Notice that we have different type of deferred work, each one associated to a bit of the bit map. To each bit is associated an entry in the softirq_vec which identifies the handler that must be activated to solve a specific type of softIRQ. Notice that lower the index entry higher is the priority of that irq. Inside the softirq_vec we can find general tasklet (one with high priority 0 and one with a normal priority 6), there are also specific tasklets associated to transmitting and receiving network packets 2 and 3, there are specific tasklets associated to read and write block of data from/to disk devices 4 and 5, another one associated to timer interrupt 1, and the 8th and 9th that are associated to internal sync of the kernel that we shall discuss later on. This bit map can be checked from three different points: after the management of a hardware interrupt, after some specific piece of codes in the kernel, or by a dedicated kernel thread ksoftirqd (a daemon) which is in charge to dispatch these activities. The check if some works have to be done is do_softIRQ(). First of all the function checks if it was called in an interrupt context (via calling in_interrupt()), if yes it simply exit, cause it will also disable interrupt but just to switch stack, the idea is that softIRQs must be interruptible (their execution not in interrupt context). Then it saves the previous RFLAGS content and disables interrupt. Then it switches to a private stack (like the hardIRQ management does) and calls \_\_do\_softIRQ() which processes pending IRQs, before returning it restore the previous content of RFLAGS re enabling also interrupts. The inner function loops on the bit map in order to check if some deferred work has been registered, if it is the case so it activates the corresponding routine found in the softirq_vec. Before activating the routine local HardIRQs are re enabled (what?), because kernel purpose is to be atomic during the stack switch and the checks performed on the bit map. The fact is since the interrupts are again enabled, it could happen that a softIRQ is interrupted cause another hardIRQ comes and registers a new softIRQ in the bit map, altering the one on which is looping the \_\_do\_softIRQ function. So in order to preserve the bit map state checked at the beginning, \_\_do\_softIRQ() makes a local copy of that! Once it finishes the loop it will check again if other hardIRQs came in the meanwhile. But this iterative approach could never finish, so it is done a fixed maximum amount of times, after that the regular execution thread is restored. But we at the same time want to solve all that pending requests, so before restoring the previous process context, the function activates the softIRQ Daemon with low priority which is in charge to make empty the bit map.

<img src=".\images\ksoftirqd.png" alt="ksoftirqd" style="zoom:80%;" />

It is an infinite loop cause it is a kernel level thread. At the beginning ksoftirqd calls explicitly the scheduler through the unique implementation independent entry point of the linux scheduler. In this way we ask the scheduler to perform its job in order to give the CPU control to one of the pending processes. Eventually it will give to this daemon the control. Why eventually? Cause it has a low priority, and since the scheduler takes in accounting also priority it might decide to schedule another "normal" process. Only when the most important thing to do is ksoftirqd, it will take CPU control. The TASK_INTERRUPTIBLE state is related to scheduler. The main idea is that if a process set that state, it will wake up if it receives a signal from other processes, otherwise it would ignore them.  If \_\_do_softirq sends a signal to the daemon, it will start to execute the next instruction from scheduler call and it will start to loop until the pending queue of deferred works is empty. The scheme is always the same, since this daemon has low priority, until do_softirq() doesn't return it will not be preemptable (steels some CPU cycles cause the system must solve a lot of softIRQs). At the end cond_resched() checks if the quantum is expired, if yes it will be descheduled otherwise it will continue to check if at least one more softIRQ is pending. 

But we know that softIRQ can be registered thanks to Tasklets data structures used to track a specific task related to the execution of a specific function in the kernel. Also drivers rely on tasklets. To register a tasklets we can pass in input an unsigned long as the pointer to the function that must be activated. The type of the function is void cause we don't know when and where this job will be executed and so we shouldn't except nothing as return parameter. We can declare tasklet relying on macros: DECLARE_TASKLET(tasklet, function, data) and DECLARE_TASKLET_DISABLED(tasklet, function, data). The only difference is that the second one won't be activated until it will be enabled manually. We can enable manually a tasklet through tasklet_enable( struct tasklet_struct *tasklet) or tasklet_hi_enable(//), these function will put in the corresponding softirq_vec entry our function in order to be executed. We can disable a tasklet through tasklet_disable(//), it means we can clear the corresponding entry. We can manually schedule the execution of a tasklet in a synchronous way through tasklet_schedule(//) (it is not expected that a programmer relies on this one). A tasklet is meant to be executed just once, so at the end it will be marked as disabled. Attention a tasklet can be executed only on a single CPU at time! If another CPU tries to execute the same tasklet (cause it was registered also on the CPU's local bit map) it will postponed again the work. This is cause it simplifies a lot the organization, avoiding the reentrant problem discussed before cause just a single CPU will execute that task at time. 

Another concept introduced after tasklets is the one related to Work Queues. The main problem about tasklets is that we don't know in what context they will be executed, because the reconciliation point could happen whenever it is convenient not taking in account the normal execution context and the possibility to delay it. This new concept is similar in spirit to tasklets but it relies on a pull of ad-hoc kernel level worker threads which will be scheduled in order to solve pending works. Since there are ad-hoc kernel level threads in charge to do that we know that all the deferred works will be executed in process context. In fact tasklets aren't meant to go to sleep, cause spinning on a lock or smtg else which will put that thread to sleep will bring a several drop of performance in the worst case, we don't know what is the context which will be delayed in case. Instead in case of having dedicated threads this option is possible, cause in the worst case the only thread that will sleep is the worker one which is executing that deferred work. They now can perform blocking operations. But pay attention to the fact that this not means that a worker thread can access userspace address space, these are threads without a counter part in userspace! The main struct that concerns with work queues is the work_struct which represents an entry of a list of works and associates to that a function pointer and data. 

<img src=".\images\Work Queues API.png" alt="Work Queues API" style="zoom:80%;" />

This is the API which allows to interact with work queues. We can initialize a work queue, we can initialize some units of deferred works, we can dynamically create and destroy some work queues, we can decide to explicitly schedule a work and also decide on which CPU do that. We can also specify an unsigned long delay which is a numerical representation of the time that our deferred work must wait before the kernel can execute it. This is a bridge between the interrupt management and the time management which we will discuss later. The delayed_work is a specific struct which contains this notion of time to wait, it contains an object of type timer_list called timer which represents in a certain way the right instance after which the kernel can execute this deferred work. Obv it contains the work to be executed and a reference to the work queue which it belongs to. 

<img src=".\images\workqueue_struct.png" alt="workqueue_struct" style="zoom:80%;" />

In the workqueue_struct there is a list of the poll of threads that can be scheduled in order to executed deferred works and a list of work queues on which these threads can search. Moreover there is a mutex to protect this structure. Since we said that worker threads can go to sleep, it could happen that some threads locked some kernel resources and therefore for a such reason went to sleep, what happens to the other worker threads that require the same locked resources? Work queue concept is not deadlock free, so the kernel implements a deadlock detection policy. It is really interesting the mechanism in which this is implemented: there is a list of so called maydays threads that if a deadlock condition is detected, they are in charge to release the resources locked by the deadlocked worker thread inform it that when it will be rescheduled it must re-execute some operation in order to re-ask some resources (it lost them). It is a common practice to use some kind of master threads that are in charge to release resources if a deadlock occurs. A common practice is not to create a work queue each time we have to post some deferred works, we can do it but it will be an overkill, cause we would create a poll of threads ready to execute a single function. Linux give us some API which wrap some event predefined work queues. We can use them without the need to specify which work queue we are interested in but just posting the work. Another tip: don't block for a long time within a deferred work, it is allow but since the functions are serialized on each CPU, we would loose a worker thread for a long period, performance would decrease. 

#### Time Management - (06-05-2020)

Each device of each nature must keep track of time passing. They can use different accuracy depending on the criticality of their applications. A computer without this subsystem would be useless. This operation must be handled by the operating systems cause their are in charge to abstract the underline hardware to users. There are multiple hardware and software facilities which concern timekeeping and each one provides different precision and granularity of the concept of time. 

The first concept about timekeeping is the one associated to the Timeouts: smtg is going wrong wrt an event. For example the TCP/IP protocol uses the concept of timeouts in order to detect the fact that a packet couldn't reach its destination due to whatever error. This detection requires a low resolution of the time and a common practice is that if smtg go in the expected way so the timeout is removed until it expires. They are basically boolean events. 

Another fundamental concept is the one of Timers: we set a certain time delay after which we ask someone to activate our function/task/whatever. This concept requires high resolution of the time, cause we want to be as precise as possible wrt time. For example, if we invoke the sleep syscall, we want to be sure that out thread of execution will sleep just the amount of time specified by the user (in a certain confidence range of course, small as well). The fact is that at the end of the delay the timer the handler will remove our timer and it will call back our specified function. 

In our hardware there are multiple subsystems which keep track of the time. Different vendors can sell different version of the hardware, each one with a certain number of components and precision about time. So there are a lot of these components, how can the kernel know about that? Of course through the BIOS, which will present at startup time to the kernel all the hardware component and their features. Kernel will classify and setup them, in order to rely on them to keep track of the time. 

The first component always present on all PCs is the Real Time Clock (RTC). It triggers a time interrupt every time a clock cycle is passed. Very often the frequency of that clock is very high, but a common practice in modern hardwares is to demultiplex it in order to absorb a lot of trash frequencies which could spawn due to the starting high frequency of the clock. It counts the number of cycles and it could trigger some event if the counter reaches a certain threshold. 
Another (more recent) component is the Time Stamp Counter (TSC) which is simply a register in which the number of cycles of the clock is stored. We can read explicitly that value relying on the RDTSC asm instruction. (Pay attention to the introduction of VDFS, a certain mechanism introduced by Intel to setup the voltages and frequencies per each core of the machine. The applications which were relying on the TSC register exploded, cause the mean of the time changed, now TSC store a value which count the passed time based on a Frequencies Store Buffer.)
Later on was introduced the Programmable Interval Timer (PIT) which can be explicitly programmed to send an interrupt periodically to all CPUs. It became very fast the most used way to implement timer interrupt, also wrt quantum time in scheduling, cause it allows to custom the duration of the quantum. It is seen by the SO like a device in which a register is accessible to set the timer value and whenever it expires it will trigger an hardwareIRQ to the SO.
Nowadays the preferred way to implement time interrupts is relying on the LAPIC circuitry. There is a Local APIC for each CPU and it can be programmed by the SO in order to manage each CPU differently. 
Finally, there is a dedicated component available which is composed by multiple timers. It is called High Precision Event Timer (HPET) in which each timer can be programmed independently, the clock source is common. 

The way in which kernel hides away the underlined timer hardware is through the subsystem of Clock Events. There are several data structures in linux kernel which group information about Clock Event Devices which are triggering some time interrupts. So these different devices are group in a single category that allows the kernel to program and manage them.

There are 5 fundamental goals for timekeeping: first of all update elapsed time since system startup, then kernel can update time and date, moreover kernel can exploit time concept to manage process scheduling (time quantum), kernel wants also to update resource usage statistics in order to know for example how much time a process required, finally it wants to check if some software timer has to fire. 

The Elapsed Time is measured by a global variable named JIFFIES, which means in slag "amount of time", so it counts the number of ticks that have occurred since the system was booted. The duration of a tick is hardware dependent so we can map it into seconds (or ms) only if we know how much is a tick. It is uses broadly in the kernel for a lot of purposes, one of them is the to use it to compare the current absolute time with the time-out value for a timer. 

Let's discuss the Timer Interrupts Management on 2.4. Why an older version? cause at the time only PIT were used and the scheme was simpler than the modern one which boiled down with a more readable code. This approach is actually used on IoTs devices cause it is the simplest according to the less computational power of those devices. These interrupts were manages like general ones according to the top/bottom half parading relying on Task Queues similar to the actual Tasklets but less versatile (they were emptied after a syscall or through the scheduler policy). The Top half executed the following actions: first of all registered the bottom half, incremented JIFFIES and then it checked if the CPU scheduler needed to be activated, if yes it raised a flag through need_resched (why using a sticky flag? cause the top half is running with IF set to 0, we don't want to perform a long scheduling task without receiving interrupts).

<img src=".\images\Timer Interrupt Activation 2.4.png" alt="Timer Interrupt Activation 2.4" style="zoom:80%;" />

This is the overall scheme behind the timer interrupt activation on 2.4. We received some timer IRQ and it triggered the timer interrupt manager.  The first thing that it did is to create a snapshot of the CPU and the call into do_IRQ. It was a wrapper in the very end of the timer interrupt handler which called do_timer function. The first thing that this function did was to increment the global variable JIFFIES. We can't know how much time was passed, we can also know the multiple number of timer IRQs which were occurred. Then do_timer updated the time of the process and decremented and checked if the counter reached 0. What is counter? Is a global variable which represents the quantum of a process. It is a multiple of the duration of a tick. If counter reached 0, do_timer set the need_resched sticky flag. After that it returned and at certain point re-enabled interrupt flag. After that kernel executed do_softirq (like the one we have already seen, it is the reconciliation point). After that the kernel checked if there is the need to reschedule, if yes it called schedule(). Coming back from the scheduler it could happen that it finished the bottom half part in another process context, different from the entering one. 

Let's have a look to the high resolution timers. They are based on the ktime_t type which has a granularity of nanosecond. So there was introduced a new struct called htimer in which  there is the possibility to manage multiple physical timers together and set the _softexpires value (the amount of time after which a registered function will be executed). Notice that the prefix "soft" means that the linux OS is not a real time system! So there is a certain interval of confidence in which this expiration is managed correctly.

<img src=".\images\High-Resolution Timers API.png" style="zoom:80%;" />

The API offered by the kernel is quite simple. We can initialize a timer, we can start or cancel it, we can check if a timer callback is running. Notice that the callback function will be activate when the timer is expired and on the core on which the timer was initialized!

There are also Kernel Timers, similar in spirit to the previous but not so precise. They are fundamental for some applications like the alarm() and they are widely used by device drivers (like the unreceived ack for a tcp packet). These timers are associated with deferrable functions (like we are saying) but there is no guarantee that at the exact time they will be executed (of course they are based on clock tick). These timers can be dynamically created and destroyed. It makes a lot of sense since these timers can be used for device drivers: if the function is executed the associated timer can be destroyed. The corresponding data structure is called timer_list in which there is a reference to the first entry which is associated to a unsigned long value called expires which identifies the amount of time after which the registered function must be activated (the same approach but without the nanoseconds granularity). 

<img src=".\images\Dynamic Kernel Timer API.png" style="zoom:80%;" />

The API offered by the kernel is always the same: we can initialize a timer, setup the expire time, modify an expire timer, delete it or check if it is still pending. A common problem is related to race conditions. Timer structures are often put in a larger data structure managed by different threads. If a timer was set during a lock on a resource, it is a good practice to delete it before releasing the resource, cause otherwise that timer would not more in our control.

After a certain number of versions, Linux had organized in a single list with nodes (slightly) ordered according to expiration time. This was a significantly unreliable and inefficient way to implement that cause to insert a node it is needed to travel over the list and so on so forth, think about how much TCP packets are produced, each one with an associated timer. So a new data structure was introduced named Timer Wheel which is a nested multi levels structure. This data structure is based on the concept of calendar queue. It is used to implement a priority queue in which, differently wrt to min heap, insertions cost O(1). A calendar queue is represented by an array in which each cell (bucket) is associated to a width (an interval of time). If we reach the last bucket, we basically wrap up to the first bucket. In each bucket we implement a linked list. Each element is associated to a time Ti so we can put it in the corresponding linked list of a bucket. Notice that we sort only the elements in the single linked list of a bucket. Notice that in the single linked list we can have buckets that belong to the current iteration or to the next (think about months: we can have a task to do in a day D at hour H which belongs to a bucket, then we can have another task to do at the same day D and hour H but in another month, this latter bucket will be put in the same linked list but it will be executed at the next month->iteration). We can observe and keep track of the time which is passing by updating the current bucket in the queue.

The Timer Wheel exploit the concept of calendar queue but implements a multi levels one.

<img src=".\images\Timer Wheel.png" alt="Timer Wheel" style="zoom:80%;" />

The idea is that we have different array at different level, in which the buckets represent a different granularity of time window. Each bucket in the level 1 is associated to a width which is a number of jiffies. All the buckets at level 1 is associated to one bucket at level 2, all the buckets at level 2 are associated to a single bucket at level 3 and so on so forth (think about seconds minutes and hours -> we have 60 seconds for each minute for each hour). It means that the sum of the widths captured by the buckets at first level is equal to the width of a single bucket at level 2 (and so on so forth). Now we don't have to worry about of different rounds in the first level cause all the tasks present are referred to the window of time of this iteration, it means that if a new event which is wrt time after the last present in this round comes, we have no longer to put it at the end of the list of the first bucket cause it will be put in the next bucket of the upper level (notice that the example refers to a timer which time expire must be put in the first bucket at level 1 of the next upper level bucket at level 2, if a timer which expires in 2 days from now will be put in the corresponding bucket at a certain level). So we will have a linked list at level 2 in the bucket after the current. This process is done every time that "the upper level bucket" is the last of the array. For example we we are pointing the last bucket at level 2 and we have to insert smtg at the end of the array at level 1, we must put it in the next bucket from the current at level 3. It is a recursive procedure until we find a free bucket. This could bring to a situation in which we would have a lot of timer in the upper levels. But the probability to have it is really small cause usually kernel spawns timers with an expires time near to the current one (ms/s/minutes). Each time we receive a timer interrupt our pointer is moving until the next bucket of course. At a certain point we will reach the last bucket at level 1 as the current one. So the next round requires to start from the head of the array but i previously empty all the array. So I move at the upper level (in the example the second one) and i move the pointer to the next 2 level bucket. Here I have all the timers which expire in this time window stored in a linked list, so I have to re ordered them in the corresponding bucket at level 1. This data structure suffers of an important problem: when we have to wrap up, we pay the cost of the re distribution of timers. In reality this cost is payed every time a "last bucket" in a level is reached. This kind of costly operation boils down in a non constant average time in time management. Moreover since the time window in which kernel will set new timers in average is quite small, when we reach the lasts buckets in a level, the probability to have a lot of timers to re distribute at the end of the current iteration is very high! This problem is verified for each level. So in the worst case we will re distribute the timers at all the levels with a cost very high. This causes a unbalanced workflow in time management. In the modern OS this implementation was likely modified in order to solve some corner cases: they tried to return to the previous concept of wrap up action in the calendar queue, it means that where possible the next timer will be put directly at the next level, leveling so the load of re balancing the timers in the corresponding buckets.

In modern versions of the kernel, to keep track of the passing time is used the APIC controller. Each time that the APIC controller sends a time interrupt, the following function is triggered.

<img src=".\images\Timer Interrupt Activation after 2.6.png" style="zoom:80%;" />

The first thing that this function does is to acknowledge the IRQ. Then it calls local_apic_timer_interrupt() which is the bridge between the received IRQ and the corresponding clock event management. After that we leave the interrupt context restoring the previous process context. 

<img src=".\images\Timer Interrupt Activation after 2.6 part 2.png" style="zoom:80%;" />

Let's look to the implementation of the bridge function. Notice that the APIC is a per cpu hardware controller, so the time interrupt will be managed at the corresponding CPU. We know that since a timer interrupt is associated to a specific CPU, in this function we will retrieve the timer interrupt (better its clock event associated) in a list called lapic_events which is a per cpu list. So after retrieved the clock event, we call the function registered inside that passing also the complete information about the event identified by the event struct itself. In the very end this handler will call update_process_time() and so on forth. This clock event management of the time is more efficient wrt number of time interrupts which must be received by the operating system. Think about to the fact that APIC controllers are programmable, instead the previous scheduling process was based on a periodic time interrupts sent by the PIT timer. Now since the APIC is programmable as we said it will send a timer interrupt only if the quantum is expired, we have no longer to rely n a global variable to decrement periodically. We will receive only timer interrupt at the end of the quantum.

POSIX compatibility tells that 6 clocks must be available to user applications, every time according to the available hardware of course. These are the following. 

<img src=".\images\POSIX Clocks.png" style="zoom:80%;" />

This is an image, which I will try to comment, in order to recap what we have said so far about timekeeping.

![](.\images\Timekeeping Architecture.png)

The main idea is that timekeeping is hardware dependent. On different architectures there are different components whose goal is to keep track of the passing time. Different components are associated to different timer devices (PIT, MAIN CLOCK, APICs... ). Depending of what are the available hardware components, the kernel will setup specific software subsystems which are used to scheduled periodic activities. 

The last interesting thing about timekeeping is the concept of Linux Watchdog. It is a software component which interacts with some hardware facilities. Its goal is to watch the behavior of the system and if smtg went so wrong it tries to recover to a normal state. In the worst situation it will reset the machine. Think about to a situation in which we don't have a physical access to the machine -> we cannot reset it manually. It is implemented in two parts in Linux: we have a kernel level module which is able to perform hard reset and an userspace background daemon that refreshes the timer when all is going normally (it means that this userspace process with some periodicity is scheduled).  It is implemented using a Non-Maskable Interrupt. Why that? Cause if the SO is stacking in a interrupt context with the IF set to 0, no one could alter execution flow.  The only goal of that NMI is to update some global counter which counts the number of NMI received. The userspace daemon will notify the kernel watchdog module via the /dev/watchdog special device file that userspace is still alive. The daemon is an infinite loop which first of all calls IOCTL (we will see that later but its idea is that calls a specific routine associated to a specific file dispatched by the ioctl syscall -> VFS facility) and then turn to sleep. The WDIOC_KEEPALIVE event simply tells the kernel module that userspace daemon is still correctly working. The NMI will check if the ioctl code was executed correctly, otherwise reset the machine.



















