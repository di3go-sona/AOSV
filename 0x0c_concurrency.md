#### Concurrency - (08-05-2020)

We talk about concurrency now cause only at this point we have an overview of the multi core systems organization. We know that each core at each moment can run kernel code and access kernel data, from the OS side we must ensure that the concurrent accesses at some kernel data structures would not bring to a corrupted state. Moreover we have already seen the interrupts management which could nest an interrupt context in a process context. This must be managed such that the context switch doesn't affect the operations on kernel data structures made by the process contexts. 

Every time we deal with concurrency we want to ensure two properties: Correctness condition and Progress condition (in distributed systems literature Safety and Liveness resp.). The first one means that nothing wrong must happen! We want that our concurrent programs to be correct. But what does it mean correct? When we design our algorithm, we think about that in a sequential way. For example a FIFO is defined by the order in which element are inserted and removed from the queue itself, First In First Out, so it implies a strict temporal ordering on the operations. If we want to implement a concurrent FIFO, it implies an ambiguous temporal ordering of course. So the easily thing that a program can do in order to manage a strict temporal ordered data structure in a concurrent program is to rely on locks or semaphores. It implies that just one process at time can access our data structure while the others are waiting for it. This resemble a sequential execution. So we can say that the concurrent FIFO implementation is SAFE cause in our example it can be associated with a sequential one, which we know its functioning. From the OS point of view, it's more complicated, cause some situation can be managed in a more flexible way without the need to insert very strict bottleneck to ensure safety. 

Focus on a simple Prod-Cons example. We know that a producer must write on the buffer if and only if it is empty and the consumer has already read the previous value. The consumer must read on the buffer if and only if the producer has produced smtg. This traditional synchronization brings a wasting of 50% of CPU clocks. It is purely sequential! Worst is to rely on spin locks in which the waiting time is a collection on CPU cycles for checking a variable that must be set by another process. Liveness so is the second face of the medal: smtg of good happens. Intuitively is the opposite of starvation cause for example if the consumer crashes while the producer is sleeping/spinlocking, it will starve forever. 

Talking about Correctness means talking about Linearizability. This concept try to map all our concurrent operations to invocations and replies (it means that if we have lot of ops in our concurrent program, we are interested in the start and the end times, no about duration). So we can construct a history of that actions.  We know that a sequential history is composed by start time and end time that are processed immediately together (like the duration was 0), knowing how our data structure works we can also analyze if the concurrent history previously built is in a certain sense "similar" to the sequential one (obv safe). How can we do that? We have to fix some order on our concurrent actions: we know that if a response happens BEFORE an invocation, these two actions cannot be reorder, they must appear in our concurrent history in same order. But if this not happens, for example a response happens after an invocation and before its response we can reorder them in which order we want. The sense is that some operation in our concurrent algorithm are strictly sequential, some others can be done in which order we want wrt to Correctness! (this approach overcomes the naif synchronization of Prod-Cons example).

Wrt Progress property we want to capture more fine grade conditions. One condition is Deadlock-free which tells that some threads (execution flows) eventually will acquire a lock, not all just a subset of them. It is quite different from condition Starvation-free which tells that EVERY thread will acquire eventually a lock. These two conditions are related to execution flows. Then Lock-free condition tells that some function call completes, on the other hand Wait-free condition tells that EVERY function call completes. To understand what these conditions mean we have to analyze the instructions that functions use. Think about RMW instructions like the Compare and Swap CAS. We want to make an update via a do-while construct (until we reach this possibility), inside this loop we read the old shared value, we perform the update local and we want to publish our update if and only if the old value is still the current one in the shared variable (no one has updated that value before us). This method is lock-free cause a subset of calls to this method will end correctly, but it can happen that if a thread is very unlikely it could starve! The wait free methods must ensure that all the calls must complete. Quite easy to make functions lock-free, not necessary doable do them wait free. The last conditions is Obstruction-free which tells that every function call completes if they execute in isolation. This is based on the transaction concept. A transaction is a data structure which encapsules our read/write operations on shared data, a little overhead which keep track of overall concurrent ops. If a transaction tries to commit, it will succeeds if and only if noone touched the same data (isolation property), otherwise it aborts. The scheme is similar to database management. It is implemented for data structure through transactional memory, via either software and hardware. There are some CPUs which implements in hardware this transaction scheme (TSX subsystem, which is implemented relying on cache coherence protocol), otherwise we can use libitm. To recap:

<img src=".\images\Progress Taxonomy.png" alt="Progress Taxonomy" style="zoom:80%;" />

Notice that progress conditions on multiprocessors architectures are about scheduling support! If our implementation is starvation-free, we have no guarantees that the scheduler is fair. 

Concurrency is a must for operating systems purposes, it will make no sense to use one core at time obv. Another thing to underline is that kernel threads can be descheduled! The kernel is PREEMPTIVE. This is required in order to have kernel processes/daemons which live only in kernel mode. Kernel code must ensure consistency and avoid deadlock, let the progress and correctness properties be satisfied. During the development of the kernel linux, a lot of solutions have been added, like explicit synchronizations (spin locks), non blocking ones or data separation like the per cpu variables. Are used also interrupt disabling and preemption disabling.

<img src=".\images\Kernel Race Conditions 1.png" alt="Kernel Race Conditions 1" style="zoom:80%;" />

Let's analyze this example. We have some task which is running in user space. At a certain point we transition in kernel mode through a syscall. While running in kernel mode we receive an interrupt request and we activate its handler. After the completion of its critical actions we return to the syscall execution. Let's suppose that both IRQ and syscall manage the shared queue. The syscall reads the number of elements present in the queue, if there is smtg it allocates some memory and pops the first element from the queue. The interrupt handler reads some data from a device and pushes that into the queue. The problem is if the handler is nested after the length call and before the branch evaluation of the syscall. In the middle the shared queue changes its state! The correctness is not take into account, cause there are no problems if the syscall doesn't read the pushed value of the interrupt value, but the application will be less responsive. 

<img src=".\images\Kernel Race Conditions 2.png" alt="Kernel Race Conditions 2"  />

Here we don't have the nested IRQ, but we have nested syscalls. Remember that kmalloc is blocking! So if the Task 1 calls the syscall which reads a value 1 from the length call and calls kmalloc, it could happen that since also kernel threads are preemptable Task 1 could fall to sleep. So scheduler may decide to schedule Task 2 which calls the very same syscall. While Task 1 is sleeping, it will read a value equal to 1 from the length call and so it also will call the kmalloc function. It could be that the kmalloc function will resolve the memory request very fast (maybe at this point the system is less loaded) and then it will process the queue element by mean the pop call. After that the Task 1 will be scheduled and it won't see the update made by Task 1, it will have the buffer allocated correctly and it will try to process an element which is no more available.

One way to fix this problem is to enable/disable preemption. The idea is that preemption might take place when the scheduler is activated. How to disable preemption so? This thing in linux is done by relying on the so called preemption counter (per cpu counter). The rule is: if the counter is non zero, the scheduler will simply return (the same process will have the control of that cpu). We can read the value of the counter with preempt_count(), we can increase by one the preemption counter with preempt_disable() and decrease that value with preempt_enable(). The latter two function need a memory barrier due to the fact that we are running in concurrent systems and we want to propagate this important change also in memory operations (every one must see the freshed value). But why we need of a counter? Cause we don't know from which function we could be called. If a function f disables preemption and the calls g, another function which disables and at the end re enables explicitly preemption, it could be that, without relying on counters, there would be the possibility that f is descheduled before it enables explicitly preemption! With the counters this thing is avoided, cause f will set to 1 the counter, then g will set it to 2 and with the re enabling of preemption by g, the counter would be 1, so we are protecting the outer function from being descheduled without its will. 

This is quite similar to the concept of enable and disable interrupts. We can manage them explicitly by relying on local_irq_disable/enable or we can ask if interrupts are already disabled through irqs_disabled. But it is very important in nested activations to use local_irq_save(flags) and local_irq_restore(flags) in order to save the previous state of the RFLAGS register in order to out of our nested level before re setting the previous state of the RFLAGS register. The saving and restoring operations are made relying on the pushf and popf asm instructions. Why we can't use counters as for preemption? Cause we can't implement that in software cause we have to interact with the firmware and so disable a "physical" bit in a register. 

Then we have the concept of per cpu variables which implement data separation in the kernel. It's the best synchrionization technique cause it doesn't need synchronization! But there are some problems which we have already seen: if a process accesses the per cpu variables on a core and it is descheduled on top of another core, the correctness is not satisfied. 

Linux kernel offers some atomic operations based on RMW instructions. In particular there is one type which is "atomic_t" (developed before it was inserted in gcc standard for userspace applications). The facilities offered are atomic_fetch_* where * stands for an arithmetic or logic operation on such data (Read Modify Write). We have also the DECLARE_BITMAP() macro which relies on this atomic type which implements a bit mask. We can set a bit, clear a bit, test and set or test and clear a bit, of course atomically. Obv this again has to deal with portability, cause we want to use architecture dependent facilities to implement RMW instructions.

We have already said that program order is different from memory order due to the out of order pipeline. So often we need memory barriers to force some reordering of memory accesses. Also compiler can reorder instructions in order to optimize usage of registers. This could be problematic if we are modifying some control register. Think about scenario in which we update the page table pointer and write it back to CR3. We want to ensure the fact that all the memory accesses to CR3 will see our update. We have two families of barriers: the optimization barriers are implemented with the barrier macro which relies on asm volatile construct in which we don't execute asm instructions but we specifies just we are clobbering memory (this is enough for the compiler to stop reordering of the instructions), otherwise if we want to solve problems related to memory accesses we have to rely on memory barriers. There are 3 different memory barriers: full mb, read rmb and write wmb memory barriers (add fences if necessary -> architecture dependent). 

The problem is that managing all scenarios on all the architectures is very difficult for kernel programmers. Usually they rely on software facilities in order to synchronize correctly concurrent execution of linux kernel. Since the kernel linux was developed firstly for single core machine, when the hardware world came out with multi core machine, kernel linux had to reinvent itself. It crashed on that architectures simply because most aspects of concurrency were not taken into account (obv there was no need at time). The first version of the kernel able to run on a multi core machine was the 2.4. Kernel programmers adopted a good engineering strategy to solve this problem. They fixed with a low performance patch the kernel in order to let it usable and then they started to develop in a more fine grain way the new multi core support. How did they patch the kernel? They introduced the Big Kernel Lock. It relies on spin lock, every time there is a context switch to transition in kernel mode, we had to wait until the giant lock give use the control (a sort of serialization of kernel accesses). 

In the linux kernel obv there are Mutexes. A mutex in linux is the implementation of a semaphore. A Mutex is declared through the DECLARE_MUTEX macro specifying its name. We can initialize it via sema_init with a certain number of token available. We can wait on it via the down function (may sleep). We can wait on a semaphore but with the possibility to be interrupted via down_interruptible (may sleep, but if an interrupt is received we will have a -EINTR error). We can also use a non blocking counter part via down_trylock which returns 0 if succeeded and it will no sleep. We can add a token to the semaphore (signal) via up function. We can rely on Mutexes since preemption, otherwise we will waste a lot of CPU time. 

The problem with mutexes is that: image that thread T1 waits on the critical section, acquires the lock and processes data. In the meanwhile thread T2 try to lock the mutex but it falls to sleep cause it is busy right now since T1. IF T1 immediately after signal the mutex, T2 could do 2 things: the first option is that scheduler didn't deschedule it and so its sleep time is really small and then it acquires the lock, or in the worst case it will be descheduled and thread T3 (a long unrelated execution) will bring on top of the core. How much time T2 must wait for being scheduled and so entering the critical section? And if another thread acquires the lock before T2 is scheduled? Very important problem for a responsive point of view when critical section are very small and accessed very often. So the alternative facility that kernel offers is that of Spin locks!

A spin lock consumes CPU cycles checking whether we can enter critical section. This is the software counter part of the busy waiting that we have in hardware. If the duration of critical sections is small like in the previous example, the waiting time is reduced wasting some CPU computational time (avoiding to go to sleep).  There are a lot of facilities offered by the kernel to manage spin locks. When we lock a spin lock we are entering a critical section, from a concurrency point of view no one will disturb us, but from the interruptibility point of view we can't assume nothing. If an interrupt comes while we are running in the critical section and its handler takes a lot of time, we are basically stretching the duration of a critical section. The number of wasted cycles would be greater.  So we have several variants of spin locks. We can lock a spin lock disabling explicitly interrupts also with the "save" variant (nested functions) through spin_lock_irqsave and spin_lock_irq. We can unlock it with the interrupt restore version through spin_unlock_irqrestore and spin_unlock_irq. We can also unlock a spin lock through spin_unlock_bh in order to identify manually a good reconciliation point. 
All the facilities to lock a spin lock will call into preempt_disable() cause transparently. This is because we are assuming that our critical section is too small to be descheduled for a small time of work. The "irq" version of lock operation will call also local_irq_disable which will set manually the interrupt flag to 0. The "irqsave" version of lock operation will call instead local_irq_save. The same reason can be done for the unlock counter part. So spin locks offer more control about preemption and interrupt management than semaphores because its usage is related to the small duration of the critical section. Notice that spin lock doesn't grantee no starvation. 

Another finer grain facility that kernel offers is Read/Write Locks. Typically we can have scenarios in which there an huge number of readers but few writers. It could be that the rate with which the data are generated is smaller than the rate with which the data are accessed. So we want to enforce the fact that multiple readers can access concurrently the data structures (we don't need just a process inside for the correctness of our algorithm). But while a writer has produced some data, it will write it in isolation, without concurrent reads (notice that it must wait for the completion of the previous reads). Just one writer can access data structures of course. We have 2 spin locks: one for reads one for writes. 

<img src=".\images\Read-Write Locks.png" alt="Read-Write Locks" style="zoom:80%;" />

Since just one writer can access the critical section, its lock/unlock primitives are quite simple. For the Get Lock algorithm for the readers we have first of all a lock on the r spin lock, which is not used to protected the entire critical section, but it locks a counter which identifies the number of readers inside the critical section (the update must be done atomically). If the reader is the first one which entered the c.s. it will lock also w spin lock (the one that prevents writers to enter c.s. if someone is reading data). After that reader can unlock the r spin lock and process data of data structure. In the release lock algorithm a reader, in order to update atomically the counter, will lock again r spin lock and decrement the counter. If it is the last reader that leaves the critical section, it means that now writers can access and so it unlocks the w spin lock. After that it unlock r spin lock to let other readers modify counter. Notice that lock on w and unlock on w can be executed from different readers! This is cause a spin lock is implicitly associated to a group of threads belonging to the same type. Here we have a scenario in which writers could starve! If readers continuously enter the critical section, the unlock w operation would be never performed.  The APIs are quite simple: we can rely on the rwlock_t type, we can ask for a read_lock_irqsave or read_unlock_irqrestore for the reader's operations or ask for an exclusive access to c.s. with write_lock_irqsave and write_unlock_irqrestore. Notice the management of interrupts.

There is another construct which is orthogonal to the previous one, called SEQLOCKS. A seqlock tries to tackle the following situation: we have small amount of data is to be protected, the data is simple (no connected structures), this data must not create side effects in the reader usage (for example we read two integers x and y, we perform the subtraction x-y and then we divide a number by that result, this could crash the thread cause it could happen that reader reads a fail value of a variable!) and finally we don't want writers to starve. The idea is that if requirements are satisfied, reader can read some fail values, but they later on will notice that and will try to reload that values! There are no sync instructions to enter as reader the c.s.
To rely on this facility we have the seqlock_t type. We can initialize it statically or dynamically. We can mark the access of a writer into the c.s. via write_seqlock and write_sequnlock. How is this seqlock implemented internally? It is simply a counter. It is atomically incremented when a writer enters and (2 times) the c.s. We are trying to exploit the concept of linearizability. The reader every time will perform a do-while loop in which calls read_seqbegin which returns a local copy of the current value of the counter of the seqlock. Then reader performs a local copy of data of interest (this is why we need small and simple data structure). Before going ahead, reader checks if the value is dirty or not using read_seqretry  passing the previous obtained seq value (the counter). If the counter was odd it means that there was a concurrent access between a reader and a writer. This is why writers increase the counter two times as we said, and an odd value means that a writer entered the c.s. but it still is there. Then it checks if the previous counter value is equal to the actual, if not it means that during the read operation a writer is entered correctly into the c.s. and exited too. If we want to manage more than one writer we have to combine different approach (just for writers, readers doesn't lock nothing).

There is something which is more elegant and fine grained: Read-Copy-Update RCU. It is the a lock free implementation of a sync algorithm. It allows also to manage pointers and it is a real gem of kernel facilities. It allows also to free some pointer to data structures also if someone else keep a reference to that! Wow! The writers in fact will know if someone is using a pointer that they want to update. It is used in most cases as a multiple reader-single writer algorithm, but with a little extension it can be applied also to a situation for multi-multi. The interesting thing is that RCU takes also into account the cache contention of synchronization variables. RCU is based on 3 different mechanism: publish-subscribe mechanism for insertion, wait for pre-existing RCU readers to complete for deletion and maintain multiple versions of RCU-updated objects for readers. The idea behind publish part is taken from software engineering: when a writer produces some new data, it has its own private copy, after that there is a certain moment in which atomically it wants to publish that in order to let it be visible to all the other threads which must be notified that an new data is published (no message passing, through low level stuff). As we have already said writers must wait for all the readers that are still keeping a previous version of such data, in order to complete the publish operation of the update for all the reader threads. This can be done cause RCU allows to keep multiple version of data, in order to let the old readers work properly and the new one to access directly the new value produced. The scope of RCU is only to manage dynamically allocated data, we can't manage static ones. Notice that no kernel control path can sleep inside a critical section protected by RCU. 

<img src=".\images\Insertion RCU.png" alt="Insertion RCU" style="zoom:80%;" />

Remember that the publish mechanism has to deal with memory order which is different from the program order! We have a definition of a struct and a global pointer gp which at the beginning points to null cause we have to dynamically allocate memory. So we ask memory to kmalloc and we locally populate the struct. The (in the previous slide from the one shown) "gp=p;" instruction try to publish our produced structure. This implementation (the one simply with gp=p) is not always correct because the local population of the struct could be done AFTER the publish operation! Someone could access invalid data. So the code which is shown in the picture above resolves this problem. We must rely on the RCU facility to publish some global shared pointer. It will boil down with the same "gp=p" instruction with a set of fences to ensure a certain memory order. 

On the other side we have a reader.

<img src=".\images\Reading RCU.png" alt="Reading RCU" style="zoom:80%;" />

The reader can't just do a p=gp assignment before the if branch because that piece of code could be significantly broken for some architectures which reorder almost everything! For example the load operation "p->a" could be done before the "p=gp" one...
So we need for portability reasons to use rcu_dereference which for some architectures will insert other memory barriers (for Intel I don't think is required, buuuuut portability). Moreover we have also to rely on rcu_read_lock and rcu_read_unlock. These are not actually locks (no spin locks no nothing). For most architectures these options will boil down just into a preemption disabling. It is quite important disable preemption during a read cause writers will wait for them! We have to prevent the fact that writers could starve. 

<img src=".\images\Wait for update RCU.png" alt="Wait for update RCU" style="zoom:80%;" />

This is an interesting picture cause shows the Grace Period during which writer, which wants to complete an update, wait for the completion of all the readers that are using an old value of the updating data. For example a writer wants to set a NULL the global pointer gp. While is removing the pointer some readers could have read that. So after the remove operation the writer will start a grace period (extended as needed) until the last of the readers which have read an old value of the pointer will finish to use it. Why it there is no need to care about of readers which reads the value of the pointer during the grace period? Cause they will see the new one! A subset will be automatically correct and we are interested in the complement subset. After all we can reclame the old value end stop the grace period sure that all the next reads will see a correct value. This thing is implemented by relying on the synchronize_rcu() function. It can be schemed  like a for each online cpu try to run on the top of that cpu. Why this? Cause rcu_read_lock, as we said, disable preemption! If the writer can run on that cpu it means that the reader has already exited from the c.s.

A lot of data structures have been re implemented in order to be RCU managed. Like a linked list which can be managed with facilities related to RCU (the ones which end with "_rcu").  For deletion in fact we can rely on list_del_rcu which will unchain the element using also memory barriers. But now on we can't simply free the pointer cause someone could actually use it. So synchronize_rcu will wait until the last reader leaves the c.s. (in this case stop to use the previous element which is being deleted). After that we can safely free that pointer. The same reasoning could be done for the update. We can't access directly and change the content of a node cause someone could read that value concurrently. So list_replace_rcu will make a new copy (dynamically allocated) of such element with the changed values, then it will chain to the list and only when synchronize_rcu will return it is safe to free the old pointer! (Multiple versions of the same element).

If we don't want writers must wait for the end of grace periods (for some reasons) , we can register a function using call_rcu() and pass that to it, in order to setup a garbage collector which at the end of the grace period will free all the memory associated to pointers no longer used (think about a deletion). The callbacks are activated by a dedicated SoftIRQ action. 